Location Optimization in R
========================================================

Harlan D. Harris and Alan Briggs

Summary: This document walks through several ways of optimizing locations
in R, given ZIP code data about peoples' home and work. Techniques include
mapping with the `ggmap` package, continuous optimization with `optim`,
and global optimization with the `DEoptim` package.

```{r setup, echo=FALSE}
library(knitr)
opts_chunk$set(fig.width=7, fig.height=5, echo=TRUE)

options(stringsAsFactors=FALSE)

library(ggplot2)
library(zipcode)
library(plyr)
library(reshape2)
library(compiler)
suppressPackageStartupMessages(library(ggmap))
suppressPackageStartupMessages(library(mapproj))
suppressPackageStartupMessages(library(DEoptim))
```

Setup
-----

First, load the ZIP code data, filtering out weird values and such. Then,
add ZIP code information, including centroid latitude and longitude.

```{r loading}
dat <- read.csv('DC2_Survey_Locations.csv')

dat <- subset(dat, Home_ZIP > 20000 & Home_ZIP < 30000 & 
                  (is.na(Work_ZIP) | (Work_ZIP > 20000 & Work_ZIP < 30000)))

dat <- mutate(dat,
              Home_ZIP = as.character(Home_ZIP),
              Work_ZIP = as.character(Work_ZIP))

names(dat) <- tolower(names(dat))

data(zipcode)
home_zip <- zipcode
names(home_zip) <- paste0("home_", names(home_zip))
work_zip <- zipcode
names(work_zip) <- paste0("work_", names(work_zip))

dat <- join(dat, home_zip, by='home_zip')
dat <- join(dat, work_zip, by='work_zip')

summary(dat)
```

1. Simple Statistics for Locations
----------------------------------

Simplest approach is to find modal, mean, and median locations.

```{r simple}

simple_mode <- names(which.max(table(dat$work_zip)))
simple_mode <- unlist(work_zip[work_zip$work_zip==simple_mode, 
                               c('work_latitude', 'work_longitude')])

simple_median <- c(median(dat$work_latitude, na.rm=TRUE),
                   median(dat$work_longitude, na.rm=TRUE))

simple_trim_mean <- c(mean(dat$work_latitude, na.rm=TRUE, trim=.1),
                      mean(dat$work_longitude, na.rm=TRUE, trim=.1))

sum_work_locs <- data.frame(type=c('Modal', 'Median', 'Trimmed Mean'),
                       lon=c(simple_mode[[2]],
                             simple_median[[2]],
                             simple_trim_mean[[2]]),
                       lat=c(simple_mode[[1]],
                             simple_median[[1]],
                             simple_trim_mean[[1]]))

print(sum_work_locs)
```

2. Mapping Locations
--------------------

Generate maps at a variety of scales, and functions to work with
them. Plot summary stat locations on the maps.

```{r map}

maps_fn = "dc_maps.Rdata"
if (file.exists(maps_fn)) {
    load(maps_fn)
} else {
    # 9 is zoomed pretty far out; 13 is pretty far in
    dc.maps <- llply(9:13,
        function(z) get_map(location=c(lon=-77.034,lat=38.901), 
                             zoom=z, source='osm', color='bw'))
    # TODO: cache this in case the server's down...
    save(dc.maps, file=maps_fn)
}

p <- ggmap(dc.maps[[4]], extent='device') + 
    geom_point(data=sum_work_locs, aes(x=lon, y=lat, color=type), size=6) +
    scale_color_brewer("Type", type='qual') +
    ggtitle("Work Location Simple Optima")
print(p)


```


3. Computing Distances and Costs
--------------------------------

We've got lats and lons, but we live on a sphere, so we need to write/steal
some convenience functions. This is the point-to-point distance from the
`fossil` package, and a vectorized version.

```{r dists}
#distance in kilometers between two long/lat positions (from "fossil" package)
earth.dist <- function (long1, lat1, long2, lat2) 
{
    rad <- pi/180
    a1 <- lat1 * rad
    a2 <- long1 * rad
    b1 <- lat2 * rad
    b2 <- long2 * rad
    dlon <- b2 - a2
    dlat <- b1 - a1
    a <- (sin(dlat/2))^2 + cos(a1) * cos(b1) * (sin(dlon/2))^2
    c <- 2 * atan2(sqrt(a), sqrt(1 - a))
    R <- 6378.145
    d <- R * c
    return(d)
}
earth.dist.o <- cmpfun(earth.dist) # make it a bit faster

# df must have cols work_longitude and work latitude
# lon and lat must be scalars
# scale just scales the units
# x=2 is cartesian distance
p2p_cost <- function(df, lon, lat, scale=100,x=2) {
    sum((earth.dist.o(df$work_longitude, df$work_latitude, lon, lat)/scale)^x, na.rm=TRUE)
}

# How good or bad are recent locations, for optimizing vs people's work
# locations?
## Recent locations funger hall, microsoft and google
funger <- c(-77.048943, 38.899316)
microsoft <- c( -77.086387, 38.962345)
google <- c(-77.027485, 38.902138)

p2p_cost_funger <- p2p_cost(dat, funger[[1]], funger[[2]], scale=100)
p2p_cost_microsoft <- p2p_cost(dat, microsoft[[1]], microsoft[[2]], scale=100)
p2p_cost_google <- p2p_cost(dat, google[[1]], google[[2]], scale=100)

print(sprintf("Point-to-point Funger cost: %0.3f", 
      p2p_cost_funger))
print(sprintf("Point-to-point Microsoft cost: %0.3f",
      p2p_cost_microsoft))
print(sprintf("Point-to-point Google cost: %0.3f",
      p2p_cost_google))

# map that
recent_locs <- data.frame(type=c('Funger', 'Microsoft', 'Google'),
                       lon=c(funger[[1]], microsoft[[1]], google[[1]]),
                       lat=c(funger[[2]], microsoft[[2]], google[[2]]),
                          cost=c(p2p_cost_funger, p2p_cost_microsoft,
                                 p2p_cost_google))

p <- ggmap(dc.maps[[4]], extent='device') + 
    geom_point(data=recent_locs, aes(x=lon, y=lat, color=cost), size=6) +
    geom_text(data=recent_locs, aes(x=lon, y=lat, color=cost, label=type), size=6, vjust=-1) +
    scale_color_continuous("Cost", low='blue', high='red') +
    ggtitle("Single-Point Cost of Recent Locations")
print(p)


```

To compute distance to a commute, need to use the "point to line
segment" distance calculation, or "point to point" if only one 
location was provided.

```{r commute_cost}

# p-norm computation. p = 2 is cartesian
dist <- function(a,b,c,d, p=2) {
    (abs(a-b)^p + abs(c-d)^p)^(1/p)
}

# distance from lon/lat to the df with four column names specified in cols
# as (end1_x, end1_y, end2_x, end2_y), where end2 may be missing.
# returns a vector of costs.
p2ls_cost_v <- function(df, cols, lon, lat, p=2, km_per_degree=69.11) {
    stopifnot(length(cols) == 4)
    stopifnot(all(cols %in% names(df)))
    stopifnot(length(lon) == 1)
    stopifnot(length(lat) == 1)
    
    #http://stackoverflow.com/questions/849211/shortest-distance-between-a-point-and-a-line-segment
    
    # convert from lat/lon to flat coordinates, using the lat/lon ratio at the point in question
    param_x = lon * km_per_degree * cos(lat)
    param_y = lat * km_per_degree
    home_x = df[[cols[[1]]]] * km_per_degree * cos(lat); home_y = df[[cols[[2]]]] * km_per_degree
    work_x = df[[cols[[3]]]] * km_per_degree * cos(lat); work_y = df[[cols[[4]]]] * km_per_degree
    
    # first, get the home distances
    home_dists <- dist(home_x, param_x, home_y, param_y, p)
    
    # then, get the distances to the line segment, which may be NA
    # length of the segment (squared)
    l2 <- (home_x - work_x)^2 + (home_y - work_y)^2
    # position of closest point on line
    t <- (param_x - home_x) * (work_x - home_x) + 
         (param_y - home_y) * (work_y - home_y)
    t <- t / l2
    proj_x <- home_x + t * (work_x - home_x)
    proj_y <- home_y + t * (work_y - home_y)
    seg_dists<- ifelse(t < 0, 
                       home_dists,
                       ifelse(t > 1,
                              dist(work_x, param_x, work_y, param_y, p),
                              dist(proj_x, param_x, proj_y, param_y, p)))

    ifelse(is.na(seg_dists), home_dists, seg_dists)
}

p2ls_cost <- function(df, lon, lat, p=2) {
    sum(p2ls_cost_v(df, c("home_longitude", "home_latitude", "work_longitude", "work_latitude"), lon, lat, p))
}

p2ls_cost_funger <- p2ls_cost(dat, funger[[1]], funger[[2]])
p2ls_cost_microsoft <- p2ls_cost(dat, microsoft[[1]], microsoft[[2]])
p2ls_cost_google <- p2ls_cost(dat, google[[1]], google[[2]])

recent_locs <- data.frame(type=c('Funger', 'Microsoft', 'Google'),
                       lon=c(funger[[1]], microsoft[[1]], google[[1]]),
                       lat=c(funger[[2]], microsoft[[2]], google[[2]]),
                          cost=c(p2ls_cost_funger, p2ls_cost_microsoft,
                                 p2ls_cost_google))

p <- ggmap(dc.maps[[4]], extent='device') + 
    geom_point(data=recent_locs, aes(x=lon, y=lat, color=cost), size=6) +
    geom_text(data=recent_locs, aes(x=lon, y=lat, color=cost, label=type), size=6, vjust=-1) +
    scale_color_continuous("Cost", low='blue', high='red') +
    ggtitle("Location-to-Commute Cost of Recent Locations")
print(p)


```

4. Mapping Single-Point Costs
-----------------------------

Iterate over a grid to get costs for various maps, then plot.

```{r p2ls_cost_maps}

make_map <- function(map) {
    bb = attr(map, "bb")
    grid <- expand.grid(lat=seq(from=bb$ll.lat, to=bb$ur.lat, length.out=41),
                        lon=seq(from=bb$ll.lon, to=bb$ur.lon, length.out=41))
    grid <- adply(grid, 1, function(rr) data.frame(cost=p2ls_cost(dat, rr$lon, rr$lat)))
    
    ggmap(map, extent='device') + 
        geom_tile(data=grid, aes(x=lon, y=lat, fill=cost, alpha=1/cost)) +
        scale_fill_gradientn("Cost", colours=rainbow(5)[5:1], trans='log') +
        scale_alpha_continuous("Cost", range=c(.1,.5)) +
        ggtitle("Distance from Commute") +
        theme(legend.position='none')
}
for (map in dc.maps[c(1,3,5)]) {
    p <- make_map(map) # handy to keep one around for later
    print(p)
}



```


5. Optimizing Single Points
---------------------------

Use simple Nelder-Mead optimization to find optimal cost.

Is it the global optimum? Seems to be -- multiple runs give the same
result.

```{r simple_p2ls_optim}

# wrap for optim()
p2ls_cost_opt <- function(params) {
    p2ls_cost(dat, params[[1]], params[[2]])
}

starting_points = list(google, microsoft, funger)
one_loc <- llply(starting_points, function(sp) optim(sp, p2ls_cost_opt, control=list(trace=0)))
print(laply(one_loc, function(ol) ol$par))

# reuse the last map...
single_p2ls_optim = list(x=one_loc[[1]]$par[[1]], y=one_loc[[1]]$par[[2]])
p + annotate('point', x=single_p2ls_optim$x, y=single_p2ls_optim$y, size=6, color='white')

```


6. Computing N-Point Costs
--------------------------

If there are multiple locations, cost is the minimum cost, for each person,
to get to any location.

```{r npoint} 

p2lsN_cost <- function(dat, latlons, p=2) {
    # for each pair of latlons, calc p2ls_cost
    costs <- laply(1:(length(latlons)/2), 
                   function(i) p2ls_cost_v(dat, 
                                           c("home_longitude", "home_latitude", "work_longitude", "work_latitude"), 
                                           latlons[[i*2-1]], latlons[[i*2]], p))
    sum(aaply(costs, 2, min))
}
p2lsN_cost_opt <- function(latlons) {
    p2lsN_cost(dat, latlons)
}

# get cost for two example triples of locations
tri1_cost <- p2lsN_cost_opt(c(google, microsoft, funger))
tri2_cost <- p2lsN_cost_opt(c(google, microsoft, single_p2ls_optim))

# bit of a hack from the geom_polygon docs to construct polygons of different colors
ids <- factor(c(1,2))
values <- data.frame(id=ids, value=c(tri1_cost, tri2_cost))
positions <- data.frame(id = rep(ids, each=3),
                        x=c(google[[1]], microsoft[[1]], funger[[1]], google[[1]], microsoft[[1]], single_p2ls_optim[[1]]),
                        y=c(google[[2]], microsoft[[2]], funger[[2]], google[[2]], microsoft[[2]], single_p2ls_optim[[2]]))
datapoly <- merge(values, positions, by="id")

# blue is better than red...
make_map(dc.maps[[4]]) + geom_polygon(data=datapoly, aes(x=x,y=y,color=value,group=id), alpha=.1, size=2) +
    scale_color_continuous(low="blue", high="red")

```


7. Local Minima in N-Point Optimization
---------------------------------------

Try that random restart with a gradient-based optimizer again. Use "L-BFGS-B" to
support bounds. Plot triangles found in 
each run on top of each other. Uh oh.

```{r localminima1, cache=TRUE}

# constrain the points to be not too far from DC
bigbox <- attr(dc.maps[[3]], "bb")
lower_box <- rep(c(bigbox$ll.lon, bigbox$ll.lat), 3)
upper_box <- rep(c(bigbox$ur.lon, bigbox$ur.lat), 3)

nruns=20

starting_points = rlply(nruns, function(i) unlist(sample(list(google, microsoft, funger), 3)) + rnorm(6, sd=.1))
three_loc <- llply(starting_points, 
                   function(sp) optim(sp, cmpfun(p2lsN_cost_opt), 
                                      method="L-BFGS-B",
                                      lower=lower_box, upper=upper_box,
                                      control=list(trace=0)))

```

```{r localminima2}
three_loc_pars <- laply(three_loc, function(ol) ol$par)
#print(three_loc_pars)

ids <- factor(1:nruns)
values <- data.frame(id=ids, value=laply(three_loc, function(x) x$value))
positions <- data.frame(id = rep(ids, each=3),
                        x=as.vector(t(three_loc_pars[,c(1,3,5)])),
                        y=as.vector(t(three_loc_pars[,c(2,4,6)])))
datapoly <- merge(values, positions, by="id")
make_map(dc.maps[[2]]) + geom_polygon(data=datapoly, aes(x=x,y=y,color=value,group=id), alpha=.1, size=2) +
    scale_color_continuous(low="blue", high="red")

```


8. Global N-Point Optimization
------------------------------

Use Differential Evolution, an algorithm that uses quasi-Darwinian 
evolution of populations of candidate solutions, to (likely) find
the global optimum.

```{r deoptim, cache=TRUE}

NP=60
init <- matrix(runif(NP*6, min=raply(NP, lower_box), max=raply(NP, upper_box)), ncol=6)
init[1,] <- c(microsoft, google, funger)

# not currently using init!
three_loc_de <- DEoptim(cmpfun(p2lsN_cost_opt), lower=lower_box, upper=upper_box,
                     control=list(NP=60, itermax=100, trace=FALSE, strategy=2))


```

9. Mapping N-Point Solutions
----------------------------

Show the DE convergence, and the global optimum on maps. The global optimum is,
roughly, Downtown + Rockville + Crystal City.

```{r deoptim_plots}
plot(three_loc_de, plot.type='bestvalit')

ids <- factor(c("Past", "Optimized"))
values <- data.frame(id=ids, value=c(tri1_cost, three_loc_de$optim$bestval))
positions <- data.frame(id = rep(ids, each=3),
                        x=c(google[[1]], microsoft[[1]], funger[[1]], three_loc_de$optim$bestmem[c(1,3,5)]),
                        y=c(google[[2]], microsoft[[2]], funger[[2]], three_loc_de$optim$bestmem[c(2, 4, 6)]))

datapoly <- merge(values, positions, by="id")
make_map(dc.maps[[3]]) + geom_polygon(data=datapoly, aes(x=x,y=y,color=value,group=id), alpha=.1, size=2) +
    scale_color_continuous(low="blue", high="red")

make_map(dc.maps[[4]]) + geom_point(data=datapoly, aes(x=x,y=y,color=value), size=5) +
    scale_color_continuous(low="blue", high="red")

make_map(dc.maps[[5]]) + geom_point(data=datapoly, aes(x=x,y=y,color=value), size=5) +
    scale_color_continuous(low="blue", high="red")

```




